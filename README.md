# ai-project
Blog: https://medium.com/p/b38eaf3b66vcb/edit
\n
A/B Testing (กราฟสีแดง) แสดงค่าเฉลี่ยของประสิทธิภาพระหว่างช่วงเวลาการทดสอบ และหลังการทดสอบ เพื่อดูการพัฒนาของตัวเลือกภาพยนตร์ที่ถูกเลือกมาแนะนำในปริมาณที่เท่ากันระหว่างการทดสอบทั้ง 10 เรื่อง หลังจากการทดสอบ พบว่า Star Wars ถูกเลือกเป็นภาพยนตร์ที่ดีที่สุด และ Optimal Value วัดจากเปอร์เซ็นต์ของ Liked
ส่วนผลของ ε-Greedy (กราฟสีฟ้า) ให้ผลดีกว่าอย่างเห็นได้ชัดเมื่อเทียบกับการทดสอบ A/B ระยะยาว และระยะสั้น ระหว่างการทดสอบของ ε-Greedy ทั้งสองการทดลอง ตามเส้นประสีฟ้าพบว่าทำได้ดีกว่าในระยะสั้น และการทดลองอื่นๆ พบว่าให้ประสิทธิภาพดีกว่าในระยะยาว ที่เป็นแบบนี้เพราะว่า ε-Greedy ใช้การ Balance ระหว่า Exploration และ Exploitation
Parameter ε ใช้บางส่วนของเวลาในการทำ Exploration และส่วนที่เลือกถูกนำมาใช้สำหรับการทำ Bandit ทำให้ ε-Greedy Bandit สามารถ Explore ได้มากขึ้น (เส้นประสีฟ้า) และรวดเร็วขึ้น เพื่อค้นหาภาพยนตร์ที่ดีที่สุด แต่ก็ยังคง Explore อย่างต่อเนื่องในการค้นหาแม้ว่าจะได้ภาพยนตร์เรื่องที่ดีที่สุดมาแล้ว จึงมีประสิทธิภาพมากกว่าในการทดสอบระยะยาว
อย่างไรก็ตามพบว่าผลลัพธ์ของ Thompson Sampling (กราฟสีน้ำตาล) ให้ผลดีสุดในการทดลองทั้งหมด Thompson Sampling ให้ผลลัพธ์ที่ดีกว่า ε-Greedy Bandit เพราะว่าสามารถปรับ Rate ในการ Explore ได้แบบ Dynamic ซึ่งดีกว่าการใช้ Rate แบบคงที่ ในขั้นเริ่มต้น Thompson Sampling จะทำการ Explore หลายๆ ครั้ง แต่เมื่อเวลาผ่านไปก็จะลดจำนวนการ Explore ลง ผลลัพธ์ที่ได้จะทำให้ Thompson Sampling สามารถระบุภาพยนตร์เรื่องที่ดีที่สุดได้เร็วกว่า และสามารถ Exploit ได้บ่อยครั้งมากกว่า ทำให้ Thompson Sampling มีประสิทธิภาพสูงกว่าทั้งระยะสั้น และระยะยาว
สรุป
จากการจำลองระบบแนะนำภาพยนตร์สามารถพิสูจน์ได้ว่าการใช้ Multi-armed Bandit Algorithm สามารถลด Regret ได้ เปอร์เซ็นต์สะสมของการแนะนำของ Bandit ที่ค่าเป็น Liked เหนือกว่าค่าเฉลี่ยของประสิทธิภาพการทดลองใน A/B Testing เราสามารถเปรียบเทียบได้ว่า A/B Testing ให้การแนะนำที่ไม่เหมาะสมจำนวนมากระหว่างการทดสอบ และต้องใช้เวลาในการแก้ไขนาน
การใช้ Bandit Algorithm อาจให้ผลลัพธ์ที่ดีกว่า ทั้งการ Converge ที่รวดเร็วกว่า หรือการทำงานที่ช้าแต่ให้ประสิทธิภาพการประมาลผลที่ดีในระยะยาว ซึ่งขึ้นกับสถานการณ์ และความต้องการจำเพาะในตอนนั้น อย่างไรก็ตาม A/B Testing ยังคงเป็นเครื่องมือที่สามารถนำมาใช้งานได้ เนื่องจาก Bandit Algorithm ลดการใช้งานตัวเลือกที่ด้อยกว่า ทำให้ต้องใช้เวลานานกว่าในการสร้างค่าสถิติที่สำคัญในการทำงานให้ได้ประสิทธิภาพ ซึ่งอาจเป็นข้อพิจารณาที่สำคัญ ท้ายที่สุดแล้วผู้ทำการทดสอบจะต้องเป็นผู้ตัดสินใจว่าจะใช้งาน Algorithm ไหนในสถานการณ์ที่กำหนด
